{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Runtime\n",
    "\n",
    "1. Why should we time our code?\n",
    "\n",
    "Comparing runtimes between two code bases, that effectively do the same thing, allows us to pick the code with the optimal performance. By gathering and analyzing runtimes, we can be sure to implement the code that is fastest and thus more efficient.\n",
    "\n",
    "2. How can we time our code?\n",
    "\n",
    "To compare runtimes, we need to be able to compute the runtime for a line or multiple lines of code. IPython comes with some handy built-in magic commands we can use to time our code. Magic commands are enhancements that have been added on top of the normal Python syntax. These commands are prefixed with the percentage sign. If you aren't familiar with magic commands, don't worry, take a moment to review the documentation using the provided link. We won't be using all the magic commands listed in the docs, but it's helpful to know what magic commands are at your disposal.\n",
    "\n",
    "    > %lsmagic\n",
    "\n",
    "3. Using %timeit\n",
    "\n",
    "Consider this example: we want to inspect the runtime for selecting 1,000 random numbers between zero and one using NumPy's random-dot-rand function. Using %timeit just requires adding the magic command before the line of code we want to analyze. That's it! One simple command to gather runtimes. Magic indeed!\n",
    "\n",
    "    > import numpy as np\n",
    "    \n",
    "    > %timeit rand_runs = np.random.rand(1000)\n",
    "\n",
    "4. %timeit output\n",
    "\n",
    "One advantage to using %timeit is the fact that it provides an average of timing statistics.\n",
    "Notice that the output provides a mean, and standard deviation, of time.\n",
    "\n",
    "    >> 6.82 µs ± 20.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
    "    \n",
    "We also see that multiple runs and loops were generated. %timeit runs through the provided code multiple times to estimate the code's execution time. This provides a more accurate representation of the actual runtime rather than relying on just one iteration to calculate the runtime. The mean and standard deviation displayed in the output is a summary of the runtime considering each of the multiple runs.\n",
    "\n",
    "5. Specifying number of runs/loops\n",
    "\n",
    "The number of runs represents how many iterations you'd like to use to estimate the runtime. The number of loops represents how many times you'd like the code to be executed per run. We can specify the number of runs, using the -r flag, and the number of loops, using the -n flag. Here, we use -r2, to set the number of runs to two and -n10, to set the number of loops to ten. In this example, %timeit would execute our random number selection 20 times in order to estimate runtime (2 runs each with 10 executions).\n",
    "\n",
    "    > %timeit -n20 -r2 rand_runs = np.random.rand(1000)\n",
    "\n",
    "6. Using %timeit in line magic mode\n",
    "\n",
    "Another cool feature of %timeit is its ability to run on single or multiple lines of code. When using %timeit in line magic mode, or with a single line of code, one percentage sign is used.\n",
    "\n",
    "    > %timeit nums = [x for x in range(10)]\n",
    "\n",
    "Similarly, we can run %timeit in cell magic mode (or provide multiple lines of code) by using two percentage signs.\n",
    "\n",
    "    > %%timeit\n",
    "    > nums = []\n",
    "    > for x in range(10):\n",
    "    >   nums.append(x)\n",
    "\n",
    "7. Saving output\n",
    "\n",
    "We can save the output of %timeit into a variable using the -o flag.\n",
    "\n",
    "    > times = %timeit -o rand_nums = np.random.rand(1000)\n",
    "    \n",
    "This allows us to dig deeper into the output and see things like the time for each run, the best time for all runs, and the worst time for all runs. Let's try using %timeit with some of Python's built-in data structures.\n",
    "\n",
    "    > times.timings\n",
    "\n",
    "    > times.best\n",
    "\n",
    "    > times.worst\n",
    "\n",
    "8. Comparing times\n",
    "\n",
    "In a previous chapter, we mentioned some of Python's built-in data structures called lists, dictionaries, and tuples. Python allows us to create these data structures using either a formal name, like list, dict, and tuple spelled out, or a shorthand called literal syntax.\n",
    "\n",
    "If we wanted to compare the runtime between creating a dictionary using the formal name and creating a dictionary using the literal syntax, we could save the output of the individual %timeit commands as shown here. Then, we could compare the two outputs.\n",
    "\n",
    "For simplicity, let's look at just the output of both the %timeit commands to see which was faster. Here, we can see that using the literal syntax to create a dictionary is faster than using the formal name without writing code to do the analysis for us.\n",
    "\n",
    "    > %timeit formal_dict = dict()\n",
    "\n",
    "    > %timeit literal_dict = {}\n",
    "\n",
    "9. Off to the races!\n",
    "\n",
    "Now that we've learned how to use %timeit, we can compare different coding approaches to select the most efficient one. It's off to the races!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%timeit rand_runs = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "#### Using %timeit: your turn!\n",
    "You'd like to create a list of integers from 0 to 50 using the range() function. However, you are unsure whether using list comprehension or unpacking the range object into a list is faster. Let's use %timeit to find the best implementation.\n",
    "\n",
    "For your convenience, a reference table of time orders of magnitude is provided below (faster at the top).\n",
    "\n",
    "symbol\tname\tunit (s)\n",
    "ns\tnanosecond\t10-9\n",
    "\n",
    "µs (us)\tmicrosecond\t10-6\n",
    "\n",
    "ms\tmillisecond\t10-3\n",
    "\n",
    "s\tsecond\t100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of integers (0-50) using list comprehension\n",
    "%timeit nums_list_comp = [num for num in range(51)]\n",
    "print(nums_list_comp)\n",
    "\n",
    "# Create a list of integers (0-50) by unpacking range\n",
    "%timeit nums_unpack = [*range(51)]\n",
    "print(nums_unpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using %timeit: specifying number of runs and loops\n",
    "A list of 480 superheroes has been loaded into your session (called heroes). You'd like to analyze the runtime for converting this heroes list into a set. Instead of relying on the default settings for %timeit, you'd like to only use 5 runs and 25 loops per each run.\n",
    "\n",
    "What is the correct syntax when using %timeit and only using 5 runs with 25 loops per each run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r5 -n25 set(heroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using %timeit: formal name or literal syntax\n",
    "Python allows you to create data structures using either a formal name or a literal syntax. In this exercise, you'll explore how using a literal syntax for creating a data structure can speed up runtimes.\n",
    "\n",
    "data structure  formal name\tliteral syntax:\n",
    "\n",
    "    > list\t        list()\t    []\n",
    "\n",
    "    > dictionary\t    dict()\t    {}\n",
    "\n",
    "    >tuple\t        tuple()\t    ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list using the formal name\n",
    "%timeit formal_list = list()\n",
    "print(formal_list)\n",
    "\n",
    "# Create a list using the literal syntax\n",
    "%timeit literal_list = []\n",
    "print(literal_list)\n",
    "\n",
    "# Print out the type of formal_list\n",
    "print(type(formal_list))\n",
    "\n",
    "# Print out the type of literal_list\n",
    "print(type(literal_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cell magic mode (%%timeit)\n",
    "From here on out, you'll be working with a superheroes dataset. For this exercise, a list of each hero's weight in kilograms (called wts) is loaded into your session. You'd like to convert these weights into pounds.\n",
    "\n",
    "You could accomplish this using the below for loop:\n",
    "\n",
    "    hero_wts_lbs = []\n",
    "        for wt in wts:\n",
    "    hero_wts_lbs.append(wt * 2.20462)\n",
    "\n",
    "Or you could use a numpy array to accomplish this task:\n",
    "\n",
    "    wts_np = np.array(wts)\n",
    "    hero_wts_lbs_np = wts_np * 2.20462\n",
    "\n",
    "Use %%timeit in your IPython console to compare runtimes between these two approaches. Make sure to press SHIFT+ENTER after the magic command to add a new line before writing the code you wish to time. After you've finished coding, answer the following question:\n",
    "\n",
    "Which of the above techniques is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "hero_wts_lbs = []\n",
    "for wt in wts:\n",
    "    hero_wts_lbs.append(wt * 2.20462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "wts_np = np.array(wts)\n",
    "hero_wts_lbs_np = wts_np * 2.20462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code profiling for runtime\n",
    "\n",
    "1. Code profiling\n",
    "\n",
    "Code profiling is a technique used to describe how long, and how often, various parts of a program are executed. The beauty of a code profiler is its ability to gather summary statistics on individual pieces of our code without using magic commands like %timeit. We'll focus on the line_profiler package to profile a function's runtime line-by-line. Since this package isn't a part of Python's Standard Library, we need to install it separately. This can easily be done with a pip install command. Let's explore using line_profiler with an example.\n",
    "\n",
    "    > pip install line_profiler\n",
    "\n",
    "2. Code profiling: runtime\n",
    "\n",
    "Suppose we have a list of superheroes along with each hero's height (in centimeters) and weight (in kilograms) loaded as NumPy arrays.\n",
    "We've also developed a function called convert_units that converts each hero's height from centimeters to inches and weight from kilograms to pounds.\n",
    "\n",
    "If we wanted to get an estimated runtime of this function, we could use %timeit. But, this will only give us the total execution time. What if we wanted to see how long each line within the function took to run?\n",
    "\n",
    "    > %timeit convert_units(heroes, hts, wts)\n",
    "\n",
    "We would have to use %timeit on each individual line of our convert_units function. But, that's a lot of manual work and not very efficient.\n",
    "\n",
    "3. Code profiling: line_profiler\n",
    "\n",
    "Instead, we can profile our function with the line_profiler package. To use this package, we first need to load it into our session. We can do this using the command %load_ext followed by line_profiler. Now, we can use the magic command %lprun, from line_profiler, to gather runtimes for individual lines of code within the convert_units function. %lprun uses a special syntax. First, we use the -f flag to indicate we'd like to profile a function.\n",
    "\n",
    "    > %load_ext line_profiler\n",
    "\n",
    "Next, we specify the name of the function we'd like to profile. Note, the name of the function is passed without any parentheses.\n",
    "\n",
    "Finally, we provide the exact function call we'd like to profile by including any arguments that are needed.\n",
    "\n",
    "    > %lprun -f convert_units convert_units(heroes, hts, wts)\n",
    "\n",
    "4. %lprun output\n",
    "\n",
    "The output from %lprun provides a nice table that summarizes the profiling statistics.\n",
    "\n",
    "First, a column specifying the line number followed by a column displaying the number of times that line was executed (called the Hits column).\n",
    "\n",
    "Next, the Time column shows the total amount of time each line took to execute.\n",
    "This column uses a specific timer unit that can be found in the first line of the output. Here, the timer unit is listed in microseconds using scientific notation.\n",
    "\n",
    "We see that line three took 13 timer units, or, roughly 13 microseconds to run. The Per Hit column gives the average amount of time spent executing a single line. This is calculated by dividing the Time column by the Hits column. Notice that line 9 was executed three times and had a total run time of three microseconds, one microsecond per hit.\n",
    "\n",
    "\n",
    "The % Time column shows the percentage of time spent on a line relative to the total amount of time spent in the function. This can be a nice way to see which lines of code are taking up the most time within a function.\n",
    "\n",
    "Finally, the source code is displayed for each line in the Line Contents column.\n",
    "\n",
    "5. %lprun output caveats\n",
    "\n",
    "You may notice that the Total time reported when using %lprun and the time reported from using %timeit do not match. Remember, %timeit uses multiple loops in order to calculate an average and standard deviation of time, so the time reported from each of these magic commands aren't expected to match exactly.\n",
    "\n",
    "6. Let's practice your new profiling powers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Using %lprun: spot bottlenecks\n",
    "Profiling a function allows you to dig deeper into the function's source code and potentially spot bottlenecks. When you see certain lines of code taking up the majority of the function's runtime, it is an indication that you may want to deploy a different, more efficient technique.\n",
    "\n",
    "Lets dig deeper into the convert_units() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units(heroes, heights, weights):\n",
    "\n",
    "    new_hts = [ht * 0.39370  for ht in heights]\n",
    "    new_wts = [wt * 2.20462  for wt in weights]\n",
    "\n",
    "    hero_data = {}\n",
    "\n",
    "    for i,hero in enumerate(heroes):\n",
    "        hero_data[hero] = (new_hts[i], new_wts[i])\n",
    "\n",
    "    return hero_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the line_profiler package into your IPython session. Then, use %lprun to profile the convert_units() function acting on your superheroes data. Remember to use the special syntax for working with %lprun (you'll have to provide a -f flag specifying the function you'd like to profile).\n",
    "\n",
    "The convert_units() function, heroes list, hts array, and wts array have been loaded into your session. After you've finished coding, answer the following question:\n",
    "\n",
    "What percentage of time is spent on the new_hts list comprehension line of code relative to the total amount of time spent in the convert_units() function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f convert_units convert_units(heroes, hts, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code profiling for memory usage\n",
    "\n",
    "1. Quick and dirty approach\n",
    "\n",
    "One basic approach for inspecting memory consumption is using Python's built-in module sys. This module contains system specific functions and contains one nice method, dot-getsizeof, that returns the size of an object in bytes. sys-dot-getsizeof is a quick and dirty way to see the size of an object. But, this only gives us the size of an individual object. What if we wanted to inspect the line-by-line memory footprint of our code? You guessed it! We'll have to use a code profiler!\n",
    "\n",
    "    > import sys\n",
    "    \n",
    "    > nums_list = [*range(1000)]\n",
    "    \n",
    "    > sys.getsizeof(nums_list)\n",
    "\n",
    "2. Code profiling: memory\n",
    "\n",
    "Just like we've used code profiling to gather detailed stats on runtimes, we can also use code profiling to analyze the memory allocation for each line of code in our code base. We'll use the memory_profiler package that is very similar to the line_profiler package. It can be downloaded via pip and comes with a handy magic command (%mprun) that uses the same syntax as %lprun.\n",
    "\n",
    "    > pip install memory_profiler\n",
    "\n",
    "    > %load_ext memory_profiler\n",
    "\n",
    "    > %mprun -f convert_units convert_units(heroes, hts, wts)\n",
    "\n",
    "One drawback to using %mprun is that any function profiled for memory consumption must be defined in a file and imported. %mprun can only be used on functions defined in physical files, and not in the IPython session. In this example, the convert_units function was placed in a file named hero_funcs-dot-py. Now, we simply import our function from the hero_funcs file and use the magic command %mprun to gather statistics on its memory footprint.\n",
    "\n",
    "    hero_funcs.py\n",
    "    \n",
    "    > from hero_functs import convert_units\n",
    "\n",
    "3. %mprun output\n",
    "\n",
    "%mprun output is similar to %lprun output. Here, we see a line-by-line description of the memory consumption for the function in question.\n",
    "\n",
    "The first column represents the line number of the code that has been profiled. The second column (Mem usage) is the memory used after that line has been executed.\n",
    "Next, the Increment column shows the difference in memory of the current line with respect to the previous one. This shows us the impact of the current line on the total memory usage.\n",
    "The last column (Line Contents) shows the source code that has been profiled.\n",
    "\n",
    "Profiling a function with %mprun allows us to see what lines are taking up a large amount of memory and possibly develop a more efficient solution. Before moving on, I want to draw your attention to a few things.\n",
    "\n",
    "4. %mprun output caveats\n",
    "\n",
    "First, the memory is reported in mebibytes. Although one mebibyte is not exactly the same as one megabyte, for our purposes, we can assume they are close enough to mean the same thing.\n",
    "\n",
    "Second, the heroes list, hts array, and wts array used to generate this output are not the original datasets we've used in the past. Instead, a random sample of 35,000 heroes was used to create these datasets in order to clearly show the power of using the memory_profiler.\n",
    "\n",
    "If we had used the original datasets, the memory used would have been too small to report. Don't worry too much about this - the main take away is that small memory allocation may not show up when using %mprun and that is a perfectly fine result. The random sample of 35,000 heroes was used solely to show typical %mprun output when the memory was large enough to report.\n",
    "\n",
    "Finally, the memory_profiler package inspects memory consumption by querying the operating system. This might be slightly different from the amount of memory that is actually used by the Python interpreter. Thus, results may differ between platforms and even between runs. Regardless, the important information can still be observed.\n",
    "\n",
    "5. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Pop quiz: steps for using %mprun\n",
    "Suppose you have a list of superheroes (named heroes) along with each hero's height (in centimeters) and weight (in kilograms) loaded as NumPy arrays (named hts and wts, respectively). You also have a convert_units() function saved in a file titled hero_funcs.py.\n",
    "\n",
    "What are the necessary steps you need to take in order to profile the convert_units() function acting on your superheroes data if you'd like to see the line-by-line memory consumption of convert_units()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the function you'd like to profile.\n",
    "from hero_funcs import convert_units\n",
    "\n",
    "# load the memory_profiler within your IPython session.\n",
    "%load_ext memory_profiler\n",
    "\n",
    "#get line-by-line memory allocations.\n",
    "%mprun -f convert_units convert_units(heroes, hts, wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using %mprun: Hero BMI\n",
    "You'd like to calculate the body mass index (BMI) for a selected sample of heroes. BMI can be calculated using the below formula:\n",
    "\n",
    "BMI = mass(kg) / height(m)^2\n",
    "\n",
    "A random sample of 25,000 superheroes has been loaded into your session as an array called sample_indices. This sample is a list of indices that corresponds to each superhero's index selected from the heroes list.\n",
    "\n",
    "A function named calc_bmi_lists has also been created and saved to a file titled bmi_lists.py. For convenience, it is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bmi_lists(sample_indices, hts, wts):\n",
    "\n",
    "    # Gather sample heights and weights as lists\n",
    "    s_hts = [hts[i] for i in sample_indices]\n",
    "    s_wts = [wts[i] for i in sample_indices]\n",
    "\n",
    "    # Convert heights from cm to m and square with list comprehension\n",
    "    s_hts_m_sqr = [(ht / 100) ** 2 for ht in s_hts]\n",
    "\n",
    "    # Calculate BMIs as a list with list comprehension\n",
    "    bmis = [s_wts[i] / s_hts_m_sqr[i] for i in range(len(sample_indices))]\n",
    "\n",
    "    return bmis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this function performs all necessary calculations using list comprehension (hence the name calc_bmi_lists()). Dig deeper into this function and analyze the memory footprint for performing your calculations using lists:\n",
    "\n",
    "* Load the memory_profiler package into your IPython session.\n",
    "* Import calc_bmi_lists from bmi_lists.\n",
    "* Once you've completed the above steps, use %mprun to profile the calc_bmi_lists() function acting on your superheroes data. The hts array and wts array have already been loaded into your session.\n",
    "* After you've finished coding, answer the following question:\n",
    "\n",
    "How much memory do the list comprehension lines of code consume in the calc_bmi_lists() function? (i.e., what is the total sum of the Increment column for these four lines of code?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "from bmi_lists import calc_bmi_lists\n",
    "%mprun -f calc_bmi_lists calc_bmi_lists(sample_indices, hts, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct! Using a list comprehension approach allocates anywhere from 0.1 MiB to 5 MiB of memory to calculate your BMIs.\n",
    "\n",
    "If you run %mprun multiple times within your session, you may notice that the Increment column reports 0.0 MiB for all lines of code. This is due to a limitation with the magic command. After running %mprun once, the memory allocation analyzed previously is taken into account for all consecutive runs and %mprun will start from the place the first run left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using %mprun: Hero BMI 2.0\n",
    "Let's see if using a different approach to calculate the BMIs can save some memory. If you remember, each hero's height and weight is stored in a numpy array. That means you can use NumPy's handy array indexing capabilities and broadcasting to perform your calculations. A function named calc_bmi_arrays has been created and saved to a file titled bmi_arrays.py. For convenience, it is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bmi_arrays(sample_indices, hts, wts):\n",
    "\n",
    "    # Gather sample heights and weights as arrays\n",
    "    s_hts = hts[sample_indices]\n",
    "    s_wts = wts[sample_indices]\n",
    "\n",
    "    # Convert heights from cm to m and square with broadcasting\n",
    "    s_hts_m_sqr = (s_hts / 100) ** 2\n",
    "\n",
    "    # Calculate BMIs as an array using broadcasting\n",
    "    bmis = s_wts / s_hts_m_sqr\n",
    "\n",
    "    return bmis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this function performs all necessary calculations using arrays.\n",
    "\n",
    "Let's see if this updated array approach decreases your memory footprint:\n",
    "\n",
    "* Load the memory_profiler package into your IPython session.\n",
    "* Import calc_bmi_arrays from bmi_arrays.\n",
    "* Once you've completed the above steps, use %mprun to profile the calc_bmi_arrays() function acting on your superheroes data. The sample_indices array, hts array, and wts array have been loaded into your session.\n",
    "* After you've finished coding, answer the following question:\n",
    "\n",
    "How much memory do the array indexing and broadcasting lines of code consume in the calc_bmi_array() function? (i.e., what is the total sum of the Increment column for these four lines of code?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmi_arrays import calc_bmi_arrays\n",
    "%load_ext memory_profiler\n",
    "%mprun -f calc_bmi_arrays calc_bmi_arrays(sample_indices, hts, wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right! By implementing an array approach, you were able to shave off a few MiBs. Although this isn't a colossal gain, it still decreases your code's overhead. If your input data grows over time, these small improvements could add up to some major efficiency gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together: Star Wars profiling\n",
    "A list of 480 superheroes has been loaded into your session (called heroes) as well as a list of each hero's corresponding publisher (called publishers).\n",
    "\n",
    "You'd like to filter the heroes list based on a hero's specific publisher, but are unsure which of the below functions is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publisher_heroes(heroes, publishers, desired_publisher):\n",
    "\n",
    "    desired_heroes = []\n",
    "\n",
    "    for i,pub in enumerate(publishers):\n",
    "        if pub == desired_publisher:\n",
    "            desired_heroes.append(heroes[i])\n",
    "\n",
    "    return desired_heroes\n",
    "def get_publisher_heroes_np(heroes, publishers, desired_publisher):\n",
    "\n",
    "    heroes_np = np.array(heroes)\n",
    "    pubs_np = np.array(publishers)\n",
    "\n",
    "    desired_heroes = heroes_np[pubs_np == desired_publisher]\n",
    "\n",
    "    return desired_heroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_publisher_heroes() to gather Star Wars heroes\n",
    "star_wars_heroes = get_publisher_heroes(heroes, publishers, 'George Lucas')\n",
    "\n",
    "print(star_wars_heroes)\n",
    "print(type(star_wars_heroes))\n",
    "\n",
    "# Use get_publisher_heroes_np() to gather Star Wars heroes\n",
    "star_wars_heroes_np = get_publisher_heroes_np(heroes, publishers, 'George Lucas')\n",
    "\n",
    "print(star_wars_heroes_np)\n",
    "print(type(star_wars_heroes_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which function has the fastest runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f get_publisher_heroes get_publisher_heroes(heroes, publishers, 'George Lucas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f get_publisher_heroes_np get_publisher_heroes_np(heroes, publishers, 'George Lucas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_publisher_heroes() function and get_publisher_heroes_np() function have been saved within a file titled hero_funcs.py (i.e., you can import both functions from hero_funcs). When using %mprun, use each function to gather the Star Wars heroes as you did in the previous step. After you've finished profiling, answer the following question:\n",
    "\n",
    "Which function uses the least amount of memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "from hero_funcs import get_publisher_heroes, get_publisher_heroes_np\n",
    "%mprun -f get_publisher_heroes get_publisher_heroes(heroes, publishers, 'George Lucas')\n",
    "%mprun -f get_publisher_heroes_np get_publisher_heroes_np(heroes, publishers, 'George Lucas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both functions have the same memory consumption."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
